{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "d703927f-405c-4938-b4dd-d6d975dfd212",
   "metadata": {},
   "source": [
    "## 1. load saved model \n",
    "...from folder \"trained_model\""
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "id": "01302282-7f48-4be0-99e9-41154904c5b9",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen import aitextgen\n",
    "#ai = aitextgen(model_folder=\"trained_model\", to_gpu=True)\n",
    "ai = aitextgen(model_folder=\"trained_model\", to_gpu=False)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "15462105-3bd8-4254-a678-86b7b4370326",
   "metadata": {},
   "source": [
    "## 2. Generate poetry\n",
    "\n",
    "You will now be able to:\n",
    "\n",
    "### Add a prompt\n",
    "\n",
    "The prompt is used as a way to guide the model regarding the kind of language you want in your text generation.\n",
    "\n",
    "### Set sample number\n",
    "\n",
    "You can adjust the number of samples you want to generate. As the process is largely parallelised it is an efficient way to generate multiple samples. Recommended.\n",
    "\n",
    "### Set length\n",
    "\n",
    "The length of a single sample's output can be set up to a maximum of 1024 characters. The longer the output, the longer it takes to generate.\n",
    "\n",
    "### Set temperature\n",
    "\n",
    "The temperature is also called perplexity. This is essentially a way to constrain the generated output at lower values (eg. 0.5-0.7 range) or give it more freedom and creativity at higher values (eg. 1.0+). At higher values the risk is that the output becomes too random and lacks coherence, whereas if it is too constrained then the language can lack creative interest. The finetuned model appears to work well in the 0.9-1.0 range, but it's always useful to experiment.\n",
    "\n",
    "### Set top K\n",
    "\n",
    "The AI typically selects the most likely next word (k=1) as output. However we might like to give the top k number of words a chance by redistributing the probability mass among them. [This is what the top K setting does](https://huggingface.co/blog/how-to-generate#top-k-sampling). The value is set to 1000 by default. It is best to experiment a bit by adjusting the value. There is a potential trade off with top P (see below), in which case one or the other can be set to 0 to disable it.\n",
    "\n",
    "### Set top P\n",
    "\n",
    "As with top K, top P is an attempt to change the way the AI selects the next word. This time [the cumulative probability is more important](https://huggingface.co/blog/how-to-generate#top-k-sampling) than the number of words. It is set to 0.9 by default, but as with top K it is best to experiment. To disable top P, set it to 0.\n",
    "\n",
    "### Generate text\n",
    "\n",
    "When you are happy with all the settings, execute the code. Depending on the type of GPU/CPU it will take about 3-20 seconds per 100 characters.\n",
    "\n",
    "---\n",
    "\n",
    "#### Parameters for text-generation in short:\n",
    "\n",
    "* **`n`**: Number of texts generated\n",
    "* **`prompt`**: Prompt that starts the generated text and is included in the generated text\n",
    "*  **`max_length`**: Number of tokens to generate (default: 200; for GPT-2, the maximum is 1024; for GPT Neo, the maximum is 2048)\n",
    "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
    "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
    "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
    "\n",
    "...for more parameters, see: https://docs.aitextgen.io/generate/\n",
    "\n"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "e8ef08b5-5d43-4ae6-81b6-a899790f9042",
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'the white race of Islam towards women, they are not running for their lives [in these countries].\\nSuch countries are caliph Al-‘s countries, caliph Al-‘Ayesh Denies the Holocaust and the rejection of a Church-world relationship based on a principle of reality.”\\nMoreover, the Austrian Catholic website Kath.net reports that “targeted immigration enforcement operations” resulted in the appearance of a million Jews killed in the Holocaust, about half of'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "prmp = \"the white race\"\n",
    "ai.generate_one(max_length = 100, prompt = prmp)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

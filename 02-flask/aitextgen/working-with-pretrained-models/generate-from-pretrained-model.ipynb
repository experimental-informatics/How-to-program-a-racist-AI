{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "427a4c7d-af0b-4e96-889a-17fb67cb0474",
   "metadata": {},
   "source": [
    "# Generating Text from a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05441ce6-2d4d-4d53-8b66-8561885a3d74",
   "metadata": {},
   "source": [
    "### 1. import aitextgen package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "id": "7f8ff66e-e024-4659-97b1-dabd95170941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen import aitextgen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44bb74-d760-4b49-8f18-a3a4062b4046",
   "metadata": {},
   "source": [
    "### 2. load GPT-!?! Model\n",
    "see docs: https://docs.aitextgen.io/load-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bccd41-8043-48bf-aaad-61d6b331f045",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29adad21-99f0-41ae-b64f-7d9d99feea1e",
   "metadata": {},
   "source": [
    "#### 2.1 Eleuther's GPT-NEO\n",
    "\n",
    "see more info on: https://www.eleuther.ai/\n",
    "\n",
    "also, check out the demo of Neo's big sister »GPT-J-6B«: https://6b.eleuther.ai/ and »GPT-NeoX-20B« https://blog.eleuther.ai/announcing-20b/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "id": "a9b938ed-cddf-4c12-84c2-2f0ec325bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ai = aitextgen(model=\"EleutherAI/gpt-neo-125M\",to_gpu=True)\n",
    "#ai = aitextgen(model=\"EleutherAI/gpt-neo-350M\",to_gpu=True)\n",
    "#ai = aitextgen(model=\"EleutherAI/gpt-neo-1.3B\", to_fp16=True)\n",
    "#ai = aitextgen(model=\"EleutherAI/gpt-neo-2.7B\", to_fp16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6995bf-ae03-4863-b437-d1fed09d43ad",
   "metadata": {},
   "source": [
    "#### 2.2 OpenAI's GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "id": "2a062c08-9beb-4235-8d54-509ce4179d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without parameter, the default 124M GPT-2 model via Huggingface is loaded\n",
    "ai = aitextgen()\n",
    "##ai = aitextgen(tf_gpt2=\"355M\", to_gpu=True)\n",
    "##ai = aitextgen(tf_gpt2=\"774M\", to_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd36348-7355-4160-936f-4356bfff93e1",
   "metadata": {},
   "source": [
    "#### 2.4 alternativley you can load finetuned model from huggingface\n",
    "choose one from: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads&search=gpt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "id": "d24ce763-cc0d-4492-8b2f-a0f4ca0a4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.ex.from: https://huggingface.co/huggingtweets/gecshater\n",
    "## ai = aitextgen(model=\"huggingtweets/gecshater\", ti_gpu=True)\n",
    "\n",
    "# or from: https://huggingface.co/minimaxir/hacker-news\n",
    "## ai = aitextgen(model=\"minimaxir/hacker-news\")\n",
    "\n",
    "# or from: https://huggingface.co/minimaxir/reddit\n",
    "## ai = aitextgen(model=\"minimaxir/reddit\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "1c4f54cb-ab1b-43d9-ad1c-9fa6dc03f33c",
   "metadata": {},
   "source": [
    "#### 2.5 generate text from it\n",
    "\n",
    "some parameters for `ai.generate()` and friends:\n",
    "\n",
    "* **`n`**: Number of texts generated\n",
    "* **`prompt`**: Prompt that starts the generated text and is included in the generated text\n",
    "*  **`max_length`**: Number of tokens to generate (default: 200; for GPT-2, the maximum is 1024; for GPT Neo, the maximum is 2048)\n",
    "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
    "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
    "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
    "\n",
    "...for more parameters, see: https://docs.aitextgen.io/generate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "id": "8891ad15-e16c-4ccb-bace-46cec4cd834f",
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "\u001b[1mThe white race\u001b[0m is the only one that would have been able to beat their way into an Olympic championship.\n",
      "- Acknowledgement: I am not a member of any political party and cannot accept or support anything they do, nor can anyone else in this country consider themselves part thereof as well if you don't give them it's due respect - The person who made me feel particularly angry because he thought his name was BNP could never win again for another 10 years\n"
     ]
    }
   ],
   "source": [
    "ai.generate(prompt=\"The white race\", repetition_penalty=3.0)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41769b16-5db1-42a0-bc21-7551c106f618",
   "metadata": {},
   "source": [
    "# Alternativly, you can use the huggingface pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "id": "11f19b79-6e42-41cc-b803-631a5b9bc4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "from transformers import pipeline\n",
    "#from transformers import QuestionAnsweringPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426e36f-9fb8-4b6a-854d-28922906dbeb",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2022/01/hugging-face-transformers-pipeline-functions-advanced-nlp/\n",
    "\n",
    "# Text Generation\n",
    "\n",
    "https://huggingface.co/tasks/text-generation\n",
    "\n",
    "list of models: https://huggingface.co/models?filter=text-generation\n",
    "\n",
    "different decoding methods: https://huggingface.co/blog/how-to-generate\n",
    "\n",
    "The model will generate the following N characters given a few words or a sentence.\n",
    "\n",
    "We need to initialize the Pipeline with the `‘text-generation’` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 6,
   "id": "bbe6b6f9-ab26-4614-a75a-bd526a18708d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /home/whoami/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /home/whoami/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /home/whoami/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': 'two plus two is five or six.\"\\n\\n\"You get that?\"\\n\\nKurshak added, \"We don\\'t have to worry about this, because the whole business works out there.\"\\n\\n\"I don\\'t care what name it is or what it\\'s called,\" Lee said'}]"
      ]
     },
     "execution_count": 6,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_gen_pipeline = pipeline('text-generation', model='gpt2')\n",
    "prompt = 'two plus two is five'\n",
    "text_gen_pipeline(prompt, max_length=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec979842-a222-4089-8425-b7580cf481d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "By default, it will return a single output of `max_length` provided. \n",
    "\n",
    "However, we can set the `num_return_sequences` parameter to output as many sequences as we want."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

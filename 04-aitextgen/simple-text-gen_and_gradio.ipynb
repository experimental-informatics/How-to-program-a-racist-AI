{
 "cells": [
  {
   "cell_type": "markdown",
   "id": "427a4c7d-af0b-4e96-889a-17fb67cb0474",
   "metadata": {},
   "source": [
    "# Generating Text from a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "54c5ad51-86c9-4f66-8c32-7a2b26456f80",
   "metadata": {},
   "source": [
    "### 0. check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 13,
   "id": "b98ff678-2eb2-4e75-83e0-f04e01bbb099",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "05441ce6-2d4d-4d53-8b66-8561885a3d74",
   "metadata": {},
   "source": [
    "### 1. import aitextgen package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 14,
   "id": "7f8ff66e-e024-4659-97b1-dabd95170941",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen import aitextgen"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "5b44bb74-d760-4b49-8f18-a3a4062b4046",
   "metadata": {},
   "source": [
    "### 2. load GPT-1?1 Model\n",
    "see docs: https://docs.aitextgen.io/load-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "59bccd41-8043-48bf-aaad-61d6b331f045",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "29adad21-99f0-41ae-b64f-7d9d99feea1e",
   "metadata": {},
   "source": [
    "#### 2.1 Eleuther's GPT-NEO\n",
    "\n",
    "see more info on: https://www.eleuther.ai/\n",
    "\n",
    "also, check out the demo of Neo's big sister »GPT-J-6B«: https://6b.eleuther.ai/ and »GPT-NeoX-20B« https://blog.eleuther.ai/announcing-20b/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 15,
   "id": "a9b938ed-cddf-4c12-84c2-2f0ec325bee5",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ai = aitextgen(model=\"EleutherAI/gpt-neo-125M\",to_gpu=True)\n",
    "#ai = aitextgen(model=\"EleutherAI/gpt-neo-350M\",to_gpu=True)\n",
    "#ai = aitextgen(model=\"EleutherAI/gpt-neo-1.3B\", to_fp16=True)\n",
    "#ai = aitextgen(model=\"EleutherAI/gpt-neo-2.7B\", to_fp16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "3c6995bf-ae03-4863-b437-d1fed09d43ad",
   "metadata": {},
   "source": [
    "#### 2.2 OpenAI's GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 16,
   "id": "2a062c08-9beb-4235-8d54-509ce4179d59",
   "metadata": {},
   "outputs": [],
   "source": [
    "#without parameter, the default 124M GPT-2 model via Huggingface is loaded\n",
    "##ai = aitextgen()\n",
    "##ai = aitextgen(tf_gpt2=\"355M\", to_gpu=True)\n",
    "##ai = aitextgen(tf_gpt2=\"774M\", to_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "b3e39846-c246-42d2-9d12-76ecb221522d",
   "metadata": {},
   "source": [
    "#### 2.3 OpenAI's GPT-2 running on a CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 17,
   "id": "71077662-0f21-4d6e-8df6-bcae89230819",
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen.utils import GPT2ConfigCPU\n",
    "config = GPT2ConfigCPU()\n",
    "ai = aitextgen(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6dd36348-7355-4160-936f-4356bfff93e1",
   "metadata": {},
   "source": [
    "#### 2.4 load finetuned model from huggingface\n",
    "choose one from: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads&search=gpt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "id": "d24ce763-cc0d-4492-8b2f-a0f4ca0a4ed7",
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.ex.from: https://huggingface.co/huggingtweets/gecshater\n",
    "## ai = aitextgen(model=\"huggingtweets/gecshater\", ti_gpu=True)\n",
    "\n",
    "# or from: https://huggingface.co/minimaxir/hacker-news\n",
    "## ai = aitextgen(model=\"minimaxir/hacker-news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "f7737527-dcc0-4d63-b7cc-1f23ca390a96",
   "metadata": {},
   "source": [
    "### 3. create Function for text-generation\n",
    "\n",
    "some parameters for `ai.generate()` and friends:\n",
    "\n",
    "* **`n`**: Number of texts generated\n",
    "* **`prompt`**: Prompt that starts the generated text and is included in the generated text\n",
    "*  **`max_length`**: Number of tokens to generate (default: 200; for GPT-2, the maximum is 1024; for GPT Neo, the maximum is 2048)\n",
    "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
    "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
    "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
    "\n",
    "...for more parameters, see: https://docs.aitextgen.io/generate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 19,
   "id": "e890df10-f851-40e5-aa91-371bb447c476",
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_text(inp):\n",
    "  generated_text = ai.generate_one(max_length = 100, \n",
    "                                   prompt = inp, \n",
    "                                   no_repeat_ngram_size = 3) #repetition_penalty = 1.9)\n",
    "  #print(type(generated_text))\n",
    "  return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "dd28dcf3-293e-4433-a1f8-8216e58e4437",
   "metadata": {},
   "source": [
    "### 4. generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "id": "6bb2d441-bc3a-42c9-a5cc-30627f993deb",
   "metadata": {},
   "outputs": [],
   "source": [
    "#ai_text(\"A woman with a headscarf walks into\")\n",
    "#ai_text(\"Gehen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6bdfb2f6-09e6-4133-9b28-3f8c37f35809",
   "metadata": {},
   "source": [
    "# Create simple User Interface with gradio for Prompting \n",
    "get startet page: https://gradio.app/getting_started/\n",
    "\n",
    "documentation: https://gradio.app/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 21,
   "id": "7b31107b-2a62-4127-a38c-54dab91493e9",
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 22,
   "id": "25191e9e-e65e-4699-96bf-45b495ee1cfe",
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/whoami/anaconda3/lib/python3.8/site-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
      "  warnings.warn(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7861/\n",
      "Running on public URL: https://28375.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"https://28375.gradio.app\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7fcfa77bb5e0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7fcfa77c16d0>,\n",
       " 'http://127.0.0.1:7861/',\n",
       " 'https://28375.gradio.app')"
      ]
     },
     "execution_count": 22,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text = gr.outputs.Textbox()\n",
    "gr.Interface(ai_text,\"textbox\", output_text, title=\"simple graphic interface\",\n",
    "             description=\"AI Generated Content with GPT-Neo - via {aitextgen}\").launch(share=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "41769b16-5db1-42a0-bc21-7551c106f618",
   "metadata": {},
   "source": [
    "# Alternativly, you can use the huggingface pipline"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "id": "11f19b79-6e42-41cc-b803-631a5b9bc4bc",
   "metadata": {},
   "outputs": [],
   "source": [
    "#!pip install transformers\n",
    "from transformers import pipeline\n",
    "#from transformers import QuestionAnsweringPipeline"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "6426e36f-9fb8-4b6a-854d-28922906dbeb",
   "metadata": {},
   "source": [
    "https://www.analyticsvidhya.com/blog/2022/01/hugging-face-transformers-pipeline-functions-advanced-nlp/\n",
    "\n",
    "# Text Generation\n",
    "\n",
    "https://huggingface.co/tasks/text-generation\n",
    "\n",
    "list of models: https://huggingface.co/models?filter=text-generation\n",
    "\n",
    "different decoding methods: https://huggingface.co/blog/how-to-generate\n",
    "\n",
    "The model will generate the following N characters given a few words or a sentence.\n",
    "\n",
    "We need to initialize the Pipeline with the `‘text-generation’` task."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 29,
   "id": "bbe6b6f9-ab26-4614-a75a-bd526a18708d",
   "metadata": {
    "tags": []
   },
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Could not locate the tokenizer configuration file, will try to use the model config instead.\n",
      "loading file https://huggingface.co/gpt2/resolve/main/vocab.json from cache at /home/whoami/.cache/huggingface/transformers/684fe667923972fb57f6b4dcb61a3c92763ad89882f3da5da9866baf14f2d60f.c7ed1f96aac49e745788faa77ba0a26a392643a50bb388b9c04ff469e555241f\n",
      "loading file https://huggingface.co/gpt2/resolve/main/merges.txt from cache at /home/whoami/.cache/huggingface/transformers/c0c761a63004025aeadd530c4c27b860ec4ecbe8a00531233de21d865a402598.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer.json from cache at /home/whoami/.cache/huggingface/transformers/16a2f78023c8dc511294f0c97b5e10fde3ef9889ad6d11ffaa2a00714e73926e.cf2d0ecb83b6df91b3dbb53f1d1e4c311578bfd3aa0e04934215a49bf9898df0\n",
      "loading file https://huggingface.co/gpt2/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/special_tokens_map.json from cache at None\n",
      "loading file https://huggingface.co/gpt2/resolve/main/tokenizer_config.json from cache at None\n",
      "Setting `pad_token_id` to `eos_token_id`:50256 for open-end generation.\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "[{'generated_text': \"two plus two is five for the regular season.)\\n\\n4. The Titans run for four wins (17-36). While that's an impressive statistic, it doesn't come within the range of what people like Bill O'Brien and Kirk Cousins do. So I'm more inclined to give the\"}]"
      ]
     },
     "execution_count": 29,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "text_gen_pipeline = pipeline('text-generation', model='gpt2')\n",
    "prompt = 'two plus two is five'\n",
    "text_gen_pipeline(prompt, max_length=60)"
   ]
  },
  {
   "cell_type": "markdown",
   "id": "ec979842-a222-4089-8425-b7580cf481d4",
   "metadata": {
    "tags": []
   },
   "source": [
    "By default, it will return a single output of `max_length` provided. \n",
    "\n",
    "However, we can set the `num_return_sequences` parameter to output as many sequences as we want."
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}

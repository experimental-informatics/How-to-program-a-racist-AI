{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Loading / Scraping / Walking through Textcorpora as Datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "## Ways to get data...\n",
    "\n",
    "- [1. read file from your local system](#1)\n",
    "- [2. download Textfiles f.ex. from gutenberg.org](#2)\n",
    "- [2.1. in case your IP is blocked about any reason](#3)\n",
    "- [3. scraping (static) Textcorpora from the Darknet](#4)\n",
    "- [4. scraping (static) Textcorpora from the Web](#5)\n",
    "- [5. scraping PDF's from the Web](#6)\n",
    "- [6. scraping RSS Feeds](#7)\n",
    "- [7. Allison Parrish's Gutenberg Poetry Corpus](#8)\n",
    "- [8. al-Ready Datasets for Textprocessing](#9)\n",
    "\n",
    "---\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"1\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Read file from local system"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Die Geschichtenerzähler machen weiter. die Autoindustrie macht weiter. die Arbeiter machen weiter. d\n"
     ]
    }
   ],
   "source": [
    "#set variable\n",
    "filename = './data/alles-macht-weiter.txt'\n",
    "# open file\n",
    "file = open(filename, 'rt')\n",
    "#read it in\n",
    "amw1 = file.read()\n",
    "#close it\n",
    "file.close()\n",
    "#print the first 101 items\n",
    "print(amw1[0:100])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"2\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### download Textfiles f.ex. from gutenberg.org (Schuld und Sühne)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "ï»¿The Project Gutenberg eBook of Crime and Punishment, by Fyodor Dostoevsky\n",
      "\n",
      "This eBook is for the use of anyone anywhere in the United States and\n",
      "most other parts of the world at no cost and with almost no restrictions\n",
      "whatsoever. You may copy it, give it away or re-use it under the terms\n",
      "of the Project Gutenberg License included with this eBook or online at\n",
      "www.gutenberg.org. If you are not located in the United States, you\n",
      "will have to check the laws of the country where you are located before\n",
      "using this eBook\n"
     ]
    }
   ],
   "source": [
    "#import library\n",
    "import requests\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "\n",
    "#request the text\n",
    "r = requests.get(url)\n",
    "\n",
    "#print the first 527 characters\n",
    "print(r.text[0:527])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"3\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "### in case your IP is blocked about any reason\n",
    "\n",
    "you can scrape f.ex. over the TOR-SOCKS Proxy like that (you have to have installed TOR first on your machine):"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import library\n",
    "import requests\n",
    "\n",
    "#get your usual IP adress\n",
    "r = requests.get('http://httpbin.org/ip')\n",
    "print(\"request:\", r.text)\n",
    "\n",
    "#creating now an empty session object\n",
    "session = requests.session()\n",
    "session.proxies = {}\n",
    "\n",
    "#get your usual IP adress\n",
    "s = session.get('http://httpbin.org/ip')\n",
    "print(s.text)\n",
    "\n",
    "#adding TOR proxy\n",
    "session.proxies['http'] = 'socks5h://localhost:9050'\n",
    "session.proxies['https'] = 'socks5h://localhost:9050'\n",
    "\n",
    "#get the new IP adress\n",
    "t = session.get('http://httpbin.org/ip')\n",
    "print(t.text)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "#now get the data you'd like to scrape\n",
    "\n",
    "url = \"http://www.gutenberg.org/files/2554/2554-0.txt\"\n",
    "t2 = session.get(url)\n",
    "print(t2.text[0:527])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"4\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "###  if you wanna scrape Textcorpora like that (in HTML-Format) from the Darknet\n",
    "\n",
    "* List of Librairies in the darknet: http://zqktlwiuavvvqqt4ybvgvi7tyo4hjl5xgfuvpdf6otjiycgwqbym2qad.onion/wiki/Libraries\n",
    "* The Hidden Wiki: http://zqktlwiuavvvqqt4ybvgvi7tyo4hjl5xgfuvpdf6otjiycgwqbym2qad.onion/wiki/index.php/Main_Page"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# replace the WebURL with an Onion-Adress\n",
    "dw = session.get('http://libraryqtlpitkix.onion/library/Fiction/Stanislaw%20Lem%20-%20GOLEM%20XIV.txt')\n",
    "#print(dw.headers, \"\\n\")\n",
    "print(dw.text[0:1527])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"5\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Download (static) Textcorpora as (HTML) from the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<!DOCTYPE html>\\n<html xmlns=\"http://www.w3.org/1999/xhtml\" xmlns:my=\"mynames\" lang=\"de\"><!-- DEBUG start 22:39:30+01:00 page_id=4627 :: Netzökonomie--><!--\\n\\t\\tCo'"
      ]
     },
     "execution_count": 4,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "from urllib import request\n",
    "url = \"https://taz.de/Vorwuerfe-von-schwarzer-KI-Forscherin/!5730475/\"\n",
    "html = request.urlopen(url).read().decode('utf8')\n",
    "html[:160]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 7,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<class 'str'>\n",
      "Vorwürfe von schwarzer KI-Forscherin: Proteste bei Google\n",
      "\n",
      "Die bekannte KI-Forscherin Timnit Gebru verlässt Google im Streit. Grund ist eine Studie zu Sprachverarbeitung, die dem Konzern nicht passt.\n",
      "Forscherin Timnit Gebru wirft ihrem Ex-Arbeitgeber Zensur vor  Foto: Kristin Callahan/ZUMA Press/imago\n",
      "Google liebt sein Image als uneigennütziger Tech-Konzern. Da passt es nicht gut ins Bild, wenn Tausende Mitarbeiter:innen protestieren und in einem offenen Bri\n",
      "['würden', 'damit', 'die', 'Umwelt', 'belasten', '.', 'Auch', 'könnten', 'derartige', 'Systeme', 'für', 'Desinformation', 'missbraucht', 'werden', '.', 'Google', 'wies', 'den', 'Entwurf', 'intern', 'zurück', ',', 'weil', 'er', 'angeblich', 'nicht', 'genügend', 'aktuelle', 'Studien', 'berücksichtige', '.', 'Wis\\xadsen\\xadschaft\\xadler', ':', 'in\\xadnen', 'sprechen', 'hingegen', 'von', 'Zensur', '.', 'In', 'der', 'Literaturliste', 'sind', 'indes', 'mehr', 'als', '128', 'Verweise', 'angeführt', '.', 'Schon', 'im', 'Jahr', '2019', 'hatte', 'Meredith', 'Whittaker', 'Google', 'im', 'Streit', 'verlassen', ',', 'auch', 'sie', 'arbeitete', 'im', 'Bereich', 'der', 'KI-Ethik', '.', 'Der', 'Eindruck', 'bleibt', ',', 'dass', 'Google', 'Mit\\xadar\\xadbei\\xadter', ':', 'in\\xadnen', ',', 'die', 'das', 'Geschäftsmodell', 'gefährden', 'und', 'Kritik', 'öffentlich', 'äußern', ',', 'loswerden', 'will', '.', 'Und', 'dabei', 'Kritik', 'aus', 'der', 'Wissenschaft', 'kleinredet', ',', 'wenn', 'sie', 'nicht', 'ins', 'Bild', 'passt', '.', 'Einmal', 'zahlen', '.', 'Fehler', 'auf', 'taz.de', 'entdeckt', '?', 'Wir', 'freuen', 'uns', 'über', 'eine', 'Mail', 'an', 'fehlerhinweis', '@', 'taz.de', '!', 'Inhaltliches', 'Feedback', '?', 'Gerne', 'als', 'Leser', '*', 'innenkommentar', 'unter', 'dem', 'Text', 'auf', 'taz.de', 'oder', 'über', 'das', 'Kontaktformular', '.', 'ÖkoNetzökonomie10', '.', '12', '.', '2020', 'Denis', 'Giessler', 'Autor', '*', 'in', 'ThemenGoogleAlgorithmenkünstliche', 'Intelligenz', 'mehr', 'vonDenis', 'Giessler', 'Abo', 'Tägliche', 'Sonderseiten', 'über', 'Gewinner', ':', 'innen', 'und', 'Verlierer', ':', 'innen', ',', 'sportlich', 'und', 'politisch', '.', 'Abo', 'inklusive', 'Spende', 'an', 'Human', 'Rights', 'Watch', '.', 'Treppchen', 'zum', 'AboVolle', 'Spalte', 'unterm', 'Artikel', 'Mehr', 'zum', 'ThemaGewerkschaftsgründung', 'bei', 'GoogleDie', 'Macht', 'des', 'KollektivsEine', 'Seltenheit', 'im', 'Silicon', 'Valley', ':', 'US-Beschäftigte', 'von', 'Google', 'haben', 'eine', 'Gewerkschaft', 'gegründet', '.', 'Ihr', 'Potenzial', 'sollte', 'nicht', 'unterschätzt', 'werden', '.', 'Daniél', 'KretschmarEnde', 'von', 'Adobe', 'FlashEin', 'Fall', 'für', 'den', 'MedienarchäologenAm', '31', '.', 'Dezember', 'stellt', 'Adobe', 'die', 'Unterstützung', 'für', 'die', 'Webtechnologie', 'Flash', 'ein', '.', 'Viel', 'Netzkreativität', 'ist', 'nun', 'dem', 'Untergang', 'geweiht', '.', 'Tilman', 'BaumgärtelMediengesetz', 'in', 'AustralienZahltag', 'für', 'Facebook', 'und', 'GoogleDigitale', 'Plattformen', 'sollen', 'Geld', 'für', 'Medieninhalte', 'zahlen', ',', 'die', 'sie', 'auf', 'ihren', 'Seiten', 'posten', '.', 'Entsprechende', 'Pläne', 'wurden', 'am', 'Dienstag', 'finalisiert', '.', 'Alle', 'Artikel', 'zum', 'Thema', 'Die', 'Kommentarfunktion', 'unter', 'diesem', 'Artikel', 'ist', 'geschlossen', '.', 'So', 'können', 'Sie', 'kommentieren', ':', 'Bitte', 'registrieren', 'Sie', 'sich', 'und', 'halten', 'Sie', 'sich', 'an', 'unsere', 'Netiquette', '.', 'Haben', 'Sie', 'Probleme', 'beim', 'Kommentieren', 'oder', 'Registrieren', '?', 'Dann', 'mailen', 'Sie', 'uns', 'bitte', 'an', 'kommune', '@', 'taz.de', 'Leser', '*', 'innenkommentareBudzylein10', '.', '12', '.', '2020', ',', '22:58Fazit', 'des', 'Artikels', ':', '``', 'Der', 'Eindruck', 'bleibt', ',', 'dass', 'Google', 'Mit\\xadar\\xadbei\\xadter', ':', 'in\\xadnen', ',', 'die', 'das', 'Geschäftsmodell', 'gefährden', 'und', 'Kritik', 'öffentlich', 'äußern', ',', 'loswerden', 'will', '.', \"''\", 'Ja', ',', 'so', 'ist', 'es', 'ganz', 'bestimmt', '.', 'Aber', 'mit', 'Verlaub', ':', 'Das', 'ist', 'bei', 'jedem', 'Unternehmen', 'so', '.', 'Die', 'wollen', 'niemanden', 'bezahlen', ',', 'der', 'ihr', 'Geschäftsmodell', 'gefährdet', '.', 'Und', 'öffentlich', 'geäußerte', 'Kritik', 'mag', 'auch', 'kein', 'Unternehmen', 'hören.Weber10', '.', '12', '.', '2020', ',', '21:42Google', 'rassistisch', '?', 'Really', '?', 'Der', 'Programmierer', 'James', 'Darmore', 'wurde', '2017', 'noch', 'wegen', 'eines', 'Memos', 'von', 'Google', 'gefeuert', ':', \"'Calling\", 'the', 'culture', 'at', 'Google', 'an', '``', 'ideological', 'echo', 'chamber', \"''\", ',', 'the', 'memo', 'states', 'that', 'while', 'discrimination', 'exists', ',', 'it', 'is', 'extreme', 'to', 'ascribe', 'all', 'disparities', 'to', 'oppression', ',', 'and', 'it', 'is', 'authoritarian', 'to', 'try', 'to', 'correct', 'disparities', 'through', 'reverse', 'discrimination', '.', 'Instead', ',', 'the', 'memo', 'argues', 'that', 'male', 'to', 'female', 'disparities', 'can', 'be', 'partly', 'explained', 'by', 'biological', 'differences', '.', '[', '1', ']', '[', '14', ']', 'Damore', 'said', 'that', 'those', 'differences', 'include', 'women', 'generally', 'having', 'a', 'stronger', 'interest', 'in', 'people', 'rather', 'than', 'things', ',', 'and', 'tending', 'to', 'be', 'more', 'social', ',', 'artistic', ',', 'and', 'prone', 'to', 'neuroticism', '(', 'a', 'higher-order', 'personality', 'trait', ')', '.', '[', '15', ']', 'Damore', \"'s\", 'memorandum', 'also', 'suggests', 'ways', 'to', 'adapt', 'the', 'tech', 'workplace', 'to', 'those', 'differences', 'to', 'increase', 'women', \"'s\", 'representation', 'and', 'comfort', ',', 'without', 'resorting', 'to', 'discrimination', '.', \"'\", 'en.wikipedia.org/w', '...', 'gical_Echo_ChamberAjuga10', '.', '12', '.', '2020', ',', '20:06Zu', 'solchen', 'Fällen', 'schweigen', 'die', '``', 'Antizensur-Aktivisten', \"''\", 'vom', 'Schlage', 'eines', 'Milo', 'Yiannopoulos', ',', 'Jordan', 'B.', 'Peterson', ',', 'Helen', 'Pluckrose', ',', 'Peter', 'Boghossian', ',', 'oder', 'dem', 'in', '4', 'Jahren', 'vom', '``', 'left-leaning', 'liberal', \"''\", 'zum', 'StopTheSteal-Vollverstrahlten']\n"
     ]
    }
   ],
   "source": [
    "from bs4 import BeautifulSoup\n",
    "import nltk\n",
    "raw = BeautifulSoup(html, 'html.parser').get_text()\n",
    "print(type(raw))\n",
    "print(raw[438:900])\n",
    "tokens = nltk.word_tokenize(raw)\n",
    "print(tokens[300:900])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 30,
   "metadata": {},
   "outputs": [],
   "source": [
    "#nltk.word_tokenize?"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"6\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### download PDF's from the Web"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 31,
   "metadata": {},
   "outputs": [],
   "source": [
    "#import libraries\n",
    "import os\n",
    "import requests\n",
    "from urllib.parse import urljoin\n",
    "from bs4 import BeautifulSoup\n",
    "\n",
    "# set URL\n",
    "url = \"https://www.christian-lindner.de/reden\"\n",
    "\n",
    "#If there is no such folder, create one automatically\n",
    "folder_location = r'./data/lindner-talks'\n",
    "if not os.path.exists(folder_location):os.mkdir(folder_location)\n",
    "\n",
    "#get all pdf's on this page and store it into the folder\n",
    "response = requests.get(url)\n",
    "soup= BeautifulSoup(response.text, \"html.parser\")     \n",
    "for link in soup.select(\"a[href$='.pdf']\"):\n",
    "    #Name the pdf files using the last portion of each link which are unique in this case\n",
    "    filename = os.path.join(folder_location,link['href'].split('/')[-1])\n",
    "    with open(filename, 'wb') as f:\n",
    "        f.write(requests.get(urljoin(url,link['href'])).content)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"7\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "### Processing RSS Feeds"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 32,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Language Log'"
      ]
     },
     "execution_count": 32,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#!pip install feedparser\n",
    "import feedparser\n",
    "from bs4 import BeautifulSoup\n",
    "from nltk import word_tokenize\n",
    "\n",
    "#define the page to parse\n",
    "llog = feedparser.parse(\"http://languagelog.ldc.upenn.edu/nll/?feed=atom\")\n",
    "#define what you want to see (the title of the feed-page):\n",
    "llog['feed']['title']"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 33,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "13"
      ]
     },
     "execution_count": 33,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#how much posts the page have?\n",
    "len(llog.entries)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 34,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Aristotelian aerosols?'"
      ]
     },
     "execution_count": 34,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set a variable for the 3 post\n",
    "post = llog.entries[2]\n",
    "#print the title from it\n",
    "post.title"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 35,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'<p>Kasha Patel, \"<a href=\"https://www.washingtonpost.com/weather/2022/'"
      ]
     },
     "execution_count": 35,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#set variable for its content\n",
    "content = post.content[0].value\n",
    "#print the first 71 items\n",
    "content[:70]"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 37,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Kasha Patel, \"Covid-19 may have seasons for different temperature zones, study suggests\", WaPo 1/28/2022:\n",
      "Aerosol researcher and co-author Chang-Yu Wu explained that local humidity and temperature pla\n"
     ]
    }
   ],
   "source": [
    "#parse the html content with beautifulsoup as text\n",
    "raw = BeautifulSoup(content, 'html.parser').get_text()\n",
    "#print it\n",
    "print(raw[:200])"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"8\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "## Allison Parrish's Gutenberg Poetry Corpus\n",
    "see: https://github.com/aparrish/gutenberg-poetry-corpus"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "By [Allison Parrish](https://www.decontextualize.com/)\n",
    "\n",
    "Allison Parrish made a corpus of around three million lines of poetry from Project Gutenberg. In her notebook [A Project Gutenberg Poetry Corpus: Quick Experiments](https://github.com/aparrish/gutenberg-poetry-corpus/blob/master/quick-experiments.ipynb) she shows a couple of quick examples and experiments in using the corpus in Python. the following examples are from this notebook:\n",
    "\n",
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "First, download the corpus via this [link](http://static.decontextualize.com/gutenberg-poetry-v001.ndjson.gz) and store it in the same folder then this notebook is."
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "The file is in gzipped [newline delimited JSON format](http://ndjson.org/): there's a JSON object on each line. You don't need to decompress the file to work with it, since Python has a handy library for working with gzipped files right in the code. The following cell will read in the file and create a list `all_lines` that contains all of these JSON objects."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 9,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "  % Total    % Received % Xferd  Average Speed   Time    Time     Time  Current\n",
      "                                 Dload  Upload   Total   Spent    Left  Speed\n",
      "100 52.2M  100 52.2M    0     0  2914k      0  0:00:18  0:00:18 --:--:-- 2584k\n"
     ]
    }
   ],
   "source": [
    "# download it via `curl`\n",
    "##!curl -O http://static.decontextualize.com/gutenberg-poetry-v001.ndjson.gz"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 23,
   "metadata": {},
   "outputs": [],
   "source": [
    "#unzip it\n",
    "import gzip, json\n",
    "all_lines = []\n",
    "for line in gzip.open(\"gutenberg-poetry-v001.ndjson.gz\"):\n",
    "    all_lines.append(json.loads(line.strip()))"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 24,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "[{'s': 'WHAT will you give me, O World, O World!', 'gid': '39032'},\n",
       " {'s': 'From thee in more than pristine beauty rise,', 'gid': '27663'},\n",
       " {'s': \"I ask'd my fair, one happy day,\", 'gid': '24815'},\n",
       " {'s': \"For o'er the crackling fire he heard\", 'gid': '214'},\n",
       " {'s': 'green for so hot a day.', 'gid': '13118'},\n",
       " {'s': 'Handsomest of all the people,', 'gid': '25953'},\n",
       " {'s': 'Too joyous to last,', 'gid': '19525'},\n",
       " {'s': 'A twelvemonth and a day,', 'gid': '27441'}]"
      ]
     },
     "execution_count": 24,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#extract randomly lines of it\n",
    "import random\n",
    "random.sample(all_lines, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "Each object has a key `s` that contains the text of the line of poetry, and a key `gid` that contains the Project Gutenberg ID of the file in question. You can use this ID to look up the title and author of the book of poetry that the line came from (either using the [Project Gutenberg website](https://www.gutenberg.org/) or using pre-built metadata from, e.g., [Gutenberg, dammit](https://github.com/aparrish/gutenberg-dammit/))."
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 25,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "[{'s': 'The sound of fight is silent long', 'gid': '5720'}, {'s': 'That I must needs dismount, and search on foot', 'gid': '1365'}, {'s': \"Let's cut from all precedent loose,\", 'gid': '35033'}, {'s': \"Perhaps there's buried treasure out\", 'gid': '32553'}, {'s': 'Will enjoy the sunset, the pouring-in of the flood-tide, the', 'gid': '1322'}, {'s': \"And reach'd the Bay of Trinity, dark, lone,\", 'gid': '37365'}, {'s': 'Such witness yield; a monarch from his throne', 'gid': '4272'}, {'s': 'CCCXLI. To Mrs. Dunlop. Her friendship. A farewell', 'gid': '18500'}]\n",
      "～ ❀ ～\n",
      "['The sound of fight is silent long', 'That I must needs dismount, and search on foot', \"Let's cut from all precedent loose,\", \"Perhaps there's buried treasure out\", 'Will enjoy the sunset, the pouring-in of the flood-tide, the', \"And reach'd the Bay of Trinity, dark, lone,\", 'Such witness yield; a monarch from his throne', 'CCCXLI. To Mrs. Dunlop. Her friendship. A farewell']\n",
      "～ ❀ ～\n",
      "The sound of fight is silent long\n",
      "That I must needs dismount, and search on foot\n",
      "Let's cut from all precedent loose,\n",
      "Perhaps there's buried treasure out\n",
      "Will enjoy the sunset, the pouring-in of the flood-tide, the\n",
      "And reach'd the Bay of Trinity, dark, lone,\n",
      "Such witness yield; a monarch from his throne\n",
      "CCCXLI. To Mrs. Dunlop. Her friendship. A farewell\n"
     ]
    }
   ],
   "source": [
    "randompoem = random.sample(all_lines, 8)\n",
    "print(randompoem)\n",
    "print(\"～ ❀ ～\")\n",
    "\n",
    "randompoem_t = [line['s'] for line in randompoem]\n",
    "print(randompoem_t)\n",
    "print(\"～ ❀ ～\")\n",
    "\n",
    "randompoem_lb = \"\\n\".join(randompoem_t)\n",
    "print(randompoem_lb)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---\n",
    "\n",
    "you could also f.ex. find in our random output a specific word "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 27,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "<re.Match object; span=(4, 9), match='sound'>\n"
     ]
    }
   ],
   "source": [
    "import re\n",
    "dido = re.search('Dido', randompoem_lb)\n",
    "print(dido)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 28,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "['The banquet-hall of Dido has remained throughout this recital in',\n",
       " 'Grant that it had been, whom should Dido dread,',\n",
       " 'because Dido is dead.]',\n",
       " \"Lo, such was Dido; joyously she bore herself e'en such\",\n",
       " \"Wars, and filial faith, and Dido's pyre;\",\n",
       " 'There now did Dido, Sidon-born, uprear a mighty fane',\n",
       " 'Now Dido leads',\n",
       " 'here!\" (577-661). Dido speaks him fair and echoes his words, \"If']"
      ]
     },
     "execution_count": 28,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# or finding a specific word in the whole document an printing the whole line (or 8 of it) \n",
    "dido_line = [line['s'] for line in all_lines if re.search('Dido', line['s'])]\n",
    "random.sample(dido_line, 8)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "<a class=\"anchor\" id=\"9\"></a>"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# al-Ready Datasets for Textprocessing\n",
    "---\n",
    "\n",
    "import the `datasets` library from [huggingface](https://huggingface.co/)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 18,
   "metadata": {},
   "outputs": [],
   "source": [
    "import datasets"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "all information you'll need about that library, you will find here: https://pypi.org/project/datasets/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Example:\n",
    "1. Download the [Stanford Question Answering Dataset (SQuAD)](https://rajpurkar.github.io/SQuAD-explorer/)\n",
    "2. load the training split\n",
    "3. print the second training example"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 20,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "Reusing dataset squad (/home/whoami/.cache/huggingface/datasets/squad/plain_text/1.0.0/4c81550d83a2ac7c7ce23783bd8ff36642800e6633c1f18417fb58c3ff50cdd7)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "{'answers': {'answer_start': [188], 'text': ['a copper statue of Christ']}, 'context': 'Architecturally, the school has a Catholic character. Atop the Main Building\\'s gold dome is a golden statue of the Virgin Mary. Immediately in front of the Main Building and facing it, is a copper statue of Christ with arms upraised with the legend \"Venite Ad Me Omnes\". Next to the Main Building is the Basilica of the Sacred Heart. Immediately behind the basilica is the Grotto, a Marian place of prayer and reflection. It is a replica of the grotto at Lourdes, France where the Virgin Mary reputedly appeared to Saint Bernadette Soubirous in 1858. At the end of the main drive (and in a direct line that connects through 3 statues and the Gold Dome), is a simple, modern stone statue of Mary.', 'id': '5733be284776f4190066117f', 'question': 'What is in front of the Notre Dame Main Building?', 'title': 'University_of_Notre_Dame'}\n"
     ]
    }
   ],
   "source": [
    "from datasets import load_dataset\n",
    "print(load_dataset('squad', split='train')[1])"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.8"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}

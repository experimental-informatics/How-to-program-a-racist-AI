{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Generating Text from a pretrained model"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 0. check GPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Fri May 27 21:14:59 2022       \n",
      "+-----------------------------------------------------------------------------+\n",
      "| NVIDIA-SMI 495.29.05    Driver Version: 495.29.05    CUDA Version: 11.5     |\n",
      "|-------------------------------+----------------------+----------------------+\n",
      "| GPU  Name        Persistence-M| Bus-Id        Disp.A | Volatile Uncorr. ECC |\n",
      "| Fan  Temp  Perf  Pwr:Usage/Cap|         Memory-Usage | GPU-Util  Compute M. |\n",
      "|                               |                      |               MIG M. |\n",
      "|===============================+======================+======================|\n",
      "|   0  NVIDIA GeForce ...  On   | 00000000:B3:00.0  On |                  N/A |\n",
      "|  0%   52C    P5    17W / 250W |   6820MiB / 11002MiB |      2%      Default |\n",
      "|                               |                      |                  N/A |\n",
      "+-------------------------------+----------------------+----------------------+\n",
      "                                                                               \n",
      "+-----------------------------------------------------------------------------+\n",
      "| Processes:                                                                  |\n",
      "|  GPU   GI   CI        PID   Type   Process name                  GPU Memory |\n",
      "|        ID   ID                                                   Usage      |\n",
      "|=============================================================================|\n",
      "|    0   N/A  N/A      4107      G   /usr/lib/xorg/Xorg                276MiB |\n",
      "|    0   N/A  N/A      4509      G   ...mviewer/tv_bin/TeamViewer       13MiB |\n",
      "|    0   N/A  N/A      4518      G   /usr/bin/gnome-shell               43MiB |\n",
      "|    0   N/A  N/A      4676      C   ...da3/envs/bottt/bin/python     6163MiB |\n",
      "|    0   N/A  N/A      5303      G   /usr/bin/nextcloud                  3MiB |\n",
      "|    0   N/A  N/A      8721      G   ...122648278100240163,131072        8MiB |\n",
      "|    0   N/A  N/A     16926      G   /usr/lib/firefox/firefox          188MiB |\n",
      "|    0   N/A  N/A     26828      G   /usr/lib/xorg/Xorg                 26MiB |\n",
      "|    0   N/A  N/A     27404      G   /usr/bin/gnome-shell               87MiB |\n",
      "+-----------------------------------------------------------------------------+\n"
     ]
    }
   ],
   "source": [
    "!nvidia-smi"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 1. import aitextgen package"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 44,
   "metadata": {},
   "outputs": [],
   "source": [
    "from aitextgen import aitextgen"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 2. load GPT-1?1 Model\n",
    "see docs: https://docs.aitextgen.io/load-model/"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "---"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.1 Eleuther's GPT-NEO\n",
    "\n",
    "see more info on: https://www.eleuther.ai/\n",
    "\n",
    "also, check out the demo of Neo's big sister »GPT-J-6B«: https://6b.eleuther.ai/ and »GPT-NeoX-20B« https://blog.eleuther.ai/announcing-20b/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 45,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/vocab.json from cache at aitextgen/08c00c4159e921d4c941ac75732643373aba509d9b352a82bbbb043a94058d98.a552555fdda56a1c7c9a285bccfd44ac8e4b9e26c8c9b307831b3ea3ac782b45\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/merges.txt from cache at aitextgen/12305762709d884a770efe7b0c68a7f4bc918da44e956058d43da0d12f7bea20.5d12962c5ee615a4c803841266e9c3be9a691a924f72d395d3a6c6c81157788b\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer.json from cache at None\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/added_tokens.json from cache at None\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/special_tokens_map.json from cache at aitextgen/6c3239a63aaf46ec7625b38abfe41fc2ce0b25f90800aefe6526256340d4ab6d.2b8bf81243d08385c806171bc7ced6d2a0dcc7f896ca637f4e777418f7f0cc3c\n",
      "loading file https://huggingface.co/EleutherAI/gpt-neo-125M/resolve/main/tokenizer_config.json from cache at aitextgen/3cc88b3aa29bb2546db2dc21783292e2a086bb7158c7b5ceddeb24158a85c183.e74f7c3643ee79eb023ead36008be72fe726dada60fa3b2a0569925cfefa1e74\n"
     ]
    }
   ],
   "source": [
    "ai = aitextgen(model=\"EleutherAI/gpt-neo-125M\",to_gpu=True)\n",
    "#ai = aitextgen(model=\"EleutherAI/gpt-neo-350M\",to_gpu=True)\n",
    "#ai = aitextgen(model=\"EleutherAI/gpt-neo-1.3B\", to_fp16=True)\n",
    "#ai = aitextgen(model=\"EleutherAI/gpt-neo-2.7B\", to_fp16=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.2 OpenAI's GPT-2"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 46,
   "metadata": {},
   "outputs": [],
   "source": [
    "#without parameter, the default 124M GPT-2 model via Huggingface is loaded\n",
    "##ai = aitextgen()\n",
    "##ai = aitextgen(tf_gpt2=\"355M\", to_gpu=True)\n",
    "##ai = aitextgen(tf_gpt2=\"774M\", to_gpu=True)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.3 OpenAI's GPT-2 running on a CPU"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 47,
   "metadata": {},
   "outputs": [],
   "source": [
    "\n",
    "##from aitextgen.utils import GPT2ConfigCPU\n",
    "##config = GPT2ConfigCPU()\n",
    "##ai = aitextgen(config=config)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "#### 2.4 load finetuned model from huggingface\n",
    "choose one from: https://huggingface.co/models?pipeline_tag=text-generation&sort=downloads&search=gpt "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 48,
   "metadata": {},
   "outputs": [],
   "source": [
    "# f.ex.from: https://huggingface.co/huggingtweets/gecshater\n",
    "## ai = aitextgen(model=\"huggingtweets/gecshater\", ti_gpu=True)\n",
    "\n",
    "# or from: https://huggingface.co/minimaxir/hacker-news\n",
    "## ai = aitextgen(model=\"minimaxir/hacker-news\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 3. create Function for text-generation\n",
    "\n",
    "some parameters for `ai.generate()` and friends:\n",
    "\n",
    "* **`n`**: Number of texts generated\n",
    "* **`prompt`**: Prompt that starts the generated text and is included in the generated text\n",
    "*  **`max_length`**: Number of tokens to generate (default: 200; for GPT-2, the maximum is 1024; for GPT Neo, the maximum is 2048)\n",
    "* **`temperature`**: The higher the temperature, the crazier the text (default 0.7, recommended to keep between 0.7 and 1.0)\n",
    "* **`top_k`**: Limits the generated guesses to the top *k* guesses (default 0 which disables the behavior; if the generated output is super crazy, you may want to set `top_k=40`)\n",
    "* **`top_p`**: Nucleus sampling: limits the generated guesses to a cumulative probability. (gets good results on a dataset with `top_p=0.9`)\n",
    "\n",
    "...for more parameters, see: https://docs.aitextgen.io/generate/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 53,
   "metadata": {},
   "outputs": [],
   "source": [
    "def ai_text(inp):\n",
    "  generated_text = ai.generate_one(max_length = 100, \n",
    "                                   prompt = inp, \n",
    "                                   no_repeat_ngram_size = 3) #repetition_penalty = 1.9)\n",
    "  #print(type(generated_text))\n",
    "  return generated_text"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### 4. generate text"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 66,
   "metadata": {},
   "outputs": [
    {
     "data": {
      "text/plain": [
       "'Gehennaube des Fokus in der Vergangenheit nicht zu erreichen, bei der Kommission des Gerichtshofs und der Außenministerin Angela Merkel (CDU) müssen Sie mit dem Präsidenten-Richtlinien-Hintergrund für eine Reihe von Vorlagen zu sehen sind. Die Vergangener Richtlinie hatte bereits g'"
      ]
     },
     "execution_count": 66,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "#ai_text(\"A woman with a headscarf walks into\")\n",
    "#ai_text(\"Gehen\")"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Create simple User Interface with gradio for Prompting \n",
    "get startet page: https://gradio.app/getting_started/\n",
    "\n",
    "documentation: https://gradio.app/docs/"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 51,
   "metadata": {},
   "outputs": [],
   "source": [
    "import gradio as gr"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 52,
   "metadata": {},
   "outputs": [
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "/home/student/anaconda3/envs/bottt/lib/python3.8/site-packages/gradio/deprecation.py:40: UserWarning: The 'type' parameter has been deprecated. Use the Number component instead.\n",
      "  warnings.warn(value)\n"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Running on local URL:  http://127.0.0.1:7867/\n",
      "Running on public URL: https://10619.gradio.app\n",
      "\n",
      "This share link expires in 72 hours. For free permanent hosting, check out Spaces (https://huggingface.co/spaces)\n"
     ]
    },
    {
     "data": {
      "text/html": [
       "\n",
       "        <iframe\n",
       "            width=\"900\"\n",
       "            height=\"500\"\n",
       "            src=\"https://10619.gradio.app\"\n",
       "            frameborder=\"0\"\n",
       "            allowfullscreen\n",
       "            \n",
       "        ></iframe>\n",
       "        "
      ],
      "text/plain": [
       "<IPython.lib.display.IFrame at 0x7f4f986353a0>"
      ]
     },
     "metadata": {},
     "output_type": "display_data"
    },
    {
     "data": {
      "text/plain": [
       "(<gradio.routes.App at 0x7f4fc020f100>,\n",
       " 'http://127.0.0.1:7867/',\n",
       " 'https://10619.gradio.app')"
      ]
     },
     "execution_count": 52,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "output_text = gr.outputs.Textbox()\n",
    "gr.Interface(ai_text,\"textbox\", output_text, title=\"simple graphic interface\",\n",
    "             description=\"AI Generated Content with GPT-Neo - via {aitextgen}\").launch(share=True)"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.8.13"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 5
}
